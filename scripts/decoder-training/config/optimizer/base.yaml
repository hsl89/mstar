# optimizer parameters
optimizer: adam
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.0e-08
adam_w_mode: true
base_learning_rate: 0.001
batch_size: 8

# learning rate scheduler parameters
lr_scheduler_type: linear
lr_min_ratio: 0.1
lr_plateau_ratio: 0.1

weight_decay: 0.05
seed: 1234
warmup_steps: 2000
