num_workers: 8
collator_output_targets: mixed_bart_clm
max_seq_length: 2048
max_output_length: 1024
clm_mix_ratio: 0.5
clm_max_doc: 4 #the clm_mix_ratio is not super-effective
training_datasets: ['/mnt/pretraining-data/package-10-04-22-v1/train1_packed_chunksize_2600.arrow','/mnt/pretraining-data/package-09-29-22-v1/train1_packed_chunksize_2600.arrow','/mnt/pretraining-data/package-09-29-22-v1/train2_packed_chunksize_2600.arrow']
validation_datasets: ['/mnt/pretraining-data/package-10-04-22-v1/val1_packed_chunksize_2600.arrow','/mnt/pretraining-data/package-09-29-22-v1/val_packed_chunksize_2600.arrow']
tokenizer: t5-base
extra_tokenizer_ids: 2000 #these can be useful in downstream tasks
new_datamodule: 1
source: 'mix_from_lists'
resume_idx: null 
mean_noise_span: 3.0
mlm_prob: 0.1565 #chosen for softmax kernel availability and multiple of 64
resume_index: 250000
keep_sentinel_ids: 0
