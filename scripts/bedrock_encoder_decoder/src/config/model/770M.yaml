ckpt_path: null
use_hf_constructor: false
return_dict: true
output_hidden_states: false
output_attentions: false
torchscript: false
torch_dtype: null
use_bfloat16: true
tie_word_embeddings: false
n_positions: 512
output_past: true
vocab_size: 34176
d_model: 1024
d_kv: 64
d_ff: 4096
num_layers: 19
num_decoder_layers: 18
num_heads: 16
dropout_rate: 0.0
layer_norm_epsilon: 0.000001
feed_forward_proj: gated-gelu
dense_act_fn: gelu
use_cache: false
gradient_checkpointing: true
use_fused_attention: true
decoder_start_token_id: 0
initializer_factor: 0.5
positional_embedding: alibi

#use fused softmax, probably redundancy here
fused_scaled_masked_softmax: true
fused_attention: true
softmax_type: mstar_fused
softmax_precision: "bf16"
