args:
- pretrain_main.py
- run_name=double_batch_size_patched_20B_stage_2_t5
- optimizer.warmup_steps=3000
- optimizer.base_learning_rate=0.000032 #doubling from 0.000016 
- trainer.limit_val_batches=50
- ++model.state_dict_path=/mnt/colehawk/bedrock_patched_state_dict/20B_pre_double/pytorch_model.bin
- deepspeed_path=config/deepspeed/bf16_zero2d.json #for p4de
- callback.save_every_n_train_steps=1000 #approximately 2hrs
- trainer.val_check_interval=999
- model.positional_embedding=alibi
- trainer.precision=bf16 
- trainer.num_sanity_val_steps=0
- data=sampled_byte_drop_5_stage_2_t5.yaml
- trainer.max_steps=5000000 #not expected to hit
- model=20B
- optimizer.micro_batch_size=2
- ++model.load_method=safe_state_dict #leave on until sharded context initialization fixed
- ++data.autotokenizer_path=/mnt/tokenizer/mstar-t5-sentencepiece-extra_ids_1920-byte_fallback/
- model.ckpt_path=null
command:
- /usr/bin/python3
image: 747303060528.dkr.ecr.us-east-1.amazonaws.com/mstar-eks:colehawk-auto-restart-11-04
name: colehawk-20B-dbl-batch-stage2
node_num: 128 #can be any number due to shard8
node_type: p4de.24xlarge
fsx: "mstar-new"
