args:
- pretrain_main.py
- run_name=resume_bart_100B
- trainer.max_steps=100000
- trainer.num_nodes=8
- optimizer.micro_batch_size=8
- optimizer.total_batch_size=512
- optimizer.base_learning_rate=0.0001
- data=bart #use bart collator
- ++data.source=unlabeled
- model.ckpt_path=/mnt_out/colehawk/easel/bart_100B/08_09_21_48/epoch\=0-step\=40000-validation_loss\=0.3450_training_loss_step\=0.3240.ckpt
- optimizer.scheduler_mult_factor=0.5
command:
- /usr/bin/python3
gpu: 8
image: 747303060528.dkr.ecr.us-east-1.amazonaws.com/mstar-eks:colehawk-easel
name: chawk-bart-100B-resume
node_num: 8
node_type: p4d.24xlarge
